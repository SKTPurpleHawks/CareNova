import torch
import os
import librosa
import numpy as np
import soundfile as sf
from faster_whisper import WhisperModel
import ctranslate2
from transformers import AutoTokenizer, AutoModelForCausalLM

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# GPU í™•ì¸
device = "cuda"
print(f"1. í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

# CUDAê°€ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸
supported_types = ctranslate2.get_supported_compute_types("cuda")
print("2. CTranslate2 CUDA ì§€ì› ì—¬ë¶€:", supported_types)

# Whisper ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
compute_type = "float32" #  "int8"
model = WhisperModel("medium", device=device, compute_type=compute_type)

def remove_silence(audio_path, output_path, threshold=20):
    # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
    y, sr = librosa.load(audio_path, sr=None)

    # ìŒì„±ì˜ ì—ë„ˆì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬´ìŒ ê°ì§€
    intervals = librosa.effects.split(y, top_db=threshold)

    # ë¬´ìŒ ì œê±° í›„ ì˜¤ë””ì˜¤ í•©ì¹˜ê¸°
    y_trimmed = np.concatenate([y[start:end] for start, end in intervals])

    # ìƒˆë¡œìš´ íŒŒì¼ ì €ì¥
    sf.write(output_path, y_trimmed, sr)
    print(f"ë¬´ìŒ ì œê±° ì™„ë£Œ! ìƒˆë¡œìš´ íŒŒì¼ ì €ì¥: {output_path}")

def save_transcription_to_txt(segments, transcription_file):

    full_text = " ".join(segment.text for segment in segments)
    #full_text = f"[{segment.start:.2f}s - {segment.end:.2f}s]: {segment.text}\n"

    with open(transcription_file, "w", encoding="utf-8") as f:
        f.write(full_text + "\n")
        f.flush()
        os.fsync(f.fileno())

    print(f"âœ… í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ! '{transcription_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# huggingface-cli login
# GPT ê¸°ë°˜ ë¬¸ì¥ ë‹¤ë“¬ê¸° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
gpt_model_name = "distilbert/distilgpt2" 
gpt_tokenizer = AutoTokenizer.from_pretrained(gpt_model_name)
gpt_model = AutoModelForCausalLM.from_pretrained(gpt_model_name, max_memory={0: "3GB"}).to(device)



def refine_text(text):
    """ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë” ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜"""
    if not text:
        print("âŒ ì˜¤ë¥˜: ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    prompt = f"ë‹¤ìŒ ë¬¸ì¥ì„ ë” ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•´ ì£¼ì„¸ìš”:\n{text}\n\nìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥:"

    # ğŸ”¹ íŒ¨ë”© í† í°ì´ ì—†ìœ¼ë©´ ì„¤ì • (GPT-2ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìŒ)
    if gpt_tokenizer.pad_token is None:
        gpt_tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # '[PAD]' í† í° ì¶”ê°€
        gpt_tokenizer.pad_token = gpt_tokenizer.eos_token  # íŒ¨ë”©ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •

    if gpt_tokenizer.pad_token_id is None:
        gpt_tokenizer.pad_token_id = gpt_tokenizer.eos_token_id  # íŒ¨ë”©ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •

    # ğŸ”¹ paddingì„ í™œì„±í™”í•˜ì—¬ input_idsë¥¼ ì–»ìŒ
    encoded_inputs = gpt_tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512)
    input_ids = encoded_inputs.input_ids.to(device)
    attention_mask = encoded_inputs.attention_mask.to(device)

    output = gpt_model.generate(
        input_ids,
        attention_mask=attention_mask,
        max_new_tokens=100,  # ğŸ”¹ ìƒˆë¡œ ìƒì„±í•  ìµœëŒ€ í† í° ê°œìˆ˜ ì„¤ì •
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=gpt_tokenizer.pad_token_id  # íŒ¨ë”© í† í° ëª…í™•íˆ ì§€ì •
    )

    refined_text = gpt_tokenizer.decode(output[0], skip_special_tokens=True)
    return refined_text



'''
def refine_text(text):
    prompt = f"ë‹¤ìŒ ë¬¸ì¥ì„ ë” ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•´ ì£¼ì„¸ìš”:\n{text}\n\nìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥:"  
    input_ids = gpt_tokenizer(prompt, return_tensors="pt").input_ids.to(device)
    if gpt_tokenizer.pad_token_id is None:
        gpt_tokenizer.pad_token_id = gpt_tokenizer.eos_token_id
    attention_mask = (input_ids != gpt_tokenizer.pad_token_id).long().to(device)
    output = gpt_model.generate(input_ids, max_length=1024, temperature=0.7, pad_token_id=gpt_tokenizer)
    refined_text = gpt_tokenizer.decode(output[0], skip_special_tokens=True)
    return refined_text
'''

input_audio = "interview1.wav"  # ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼
output_audio = "interview1.wav"  # ë¬´ìŒ ì œê±° í›„ ì €ì¥ë  íŒŒì¼
transcription_file = "interview1.txt"
refined_file = "interview1_r.txt"


# ë¬´ìŒ ì œê±° ì‹¤í–‰
# remove_silence(input_audio, output_audio)


# ìŒì„± íŒŒì¼ ë³€í™˜ ì‹¤í–‰ (íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸)
'''
if os.path.exists(output_audio):
    print("ìŒì„± ë³€í™˜ ì‹œì‘...")
    # ìŒì„± íŒŒì¼ ë³€í™˜
    segments, _ = model.transcribe(output_audio)

    if not segments:
        print("ë²ˆì—­ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ê²°ê³¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œ ì¶”ê°€í•˜ê¸°

        save_transcription_to_txt(segments, transcription_file)
else:
    print("ì˜¤ë¥˜: ë³€í™˜ëœ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ í…ìŠ¤íŠ¸ ë³€í™˜ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
'''

def read_text_from_file(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()  # íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì½ê¸°
        return text
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

text = read_text_from_file(transcription_file)
refine_text(text)

with open(refined_file, "w", encoding="utf-8") as f:
    f.write(refined_file)